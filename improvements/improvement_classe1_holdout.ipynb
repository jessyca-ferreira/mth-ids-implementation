{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mj_TCXz1pRZn"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "mj_TCXz1pRZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo_Hi9Q7or51",
        "outputId": "175a8085-97e0-4eb0-dabc-388d43f353cc"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimizer in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.5.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (25.7.0)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimizer) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->scikit-optimizer) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "hUFr_cMRmST7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score,roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "9x_fFBnfmo9u"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train test split"
      ],
      "metadata": {
        "id": "-RgYN5zepWWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('./CICIDS2017_sample_km.csv')"
      ],
      "metadata": {
        "id": "c-HlLSyxmpXj"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "KfAU5Aq2mrVt",
        "outputId": "59428b87-a0f1-47bf-d293-4100b6d7022c"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "0     10226\n",
              "4      4619\n",
              "10     3178\n",
              "2      2564\n",
              "1      1966\n",
              "12     1507\n",
              "14      652\n",
              "3       208\n",
              "7       155\n",
              "6       118\n",
              "11      116\n",
              "5       113\n",
              "9        36\n",
              "13       21\n",
              "8        11\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Label'] == 1, 'Label'] = 15\n",
        "mask = (df['Label'] != 0) & (df['Label'] != 15)\n",
        "df.loc[mask, 'Label'] = 1\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "oeQhLxDw3u5X"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "M-Ne5x6031Dx",
        "outputId": "b3ad08d9-f06b-4b4e-cb4c-858e18623144"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "1     13298\n",
              "0     10226\n",
              "15     1966\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1966</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop(['Label'],axis=1).dtypes[df.dtypes != 'object'].index\n",
        "df[features] = df[features].apply(\n",
        "    lambda x: (x - x.mean()) / (x.std()))\n",
        "\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df = df.fillna(0)"
      ],
      "metadata": {
        "id": "VcMt7ZWGmr0N"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Label', axis='columns')\n",
        "y = df['Label']\n",
        "y=np.ravel(y)\n",
        "pd.Series(y).value_counts()"
      ],
      "metadata": {
        "id": "mERlNXaLnE2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "02448962-167f-4810-b350-53dcb0c5e22d"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1     13298\n",
              "0     10226\n",
              "15     1966\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1966</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering (IG, FCBF, and KPCA)"
      ],
      "metadata": {
        "id": "-csDS2vWmypc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "importances = mutual_info_classif(X, y)"
      ],
      "metadata": {
        "id": "HhJ2GYybmx3l"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the sum of importance scores\n",
        "f_list = sorted(zip(map(lambda x: round(x, 4), importances), features), reverse=True)\n",
        "Sum = 0\n",
        "fs = []\n",
        "for i in range(0, len(f_list)):\n",
        "    Sum = Sum + f_list[i][0]\n",
        "    fs.append(f_list[i][1])"
      ],
      "metadata": {
        "id": "Z8ceC_GQnHPM"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the important features from top to bottom until the accumulated importance reaches 90%\n",
        "f_list2 = sorted(zip(map(lambda x: round(x, 4), importances/Sum), features), reverse=True)\n",
        "Sum2 = 0\n",
        "fs = []\n",
        "for i in range(0, len(f_list2)):\n",
        "    Sum2 = Sum2 + f_list2[i][0]\n",
        "    fs.append(f_list2[i][1])\n",
        "    if Sum2>=0.9:\n",
        "        break"
      ],
      "metadata": {
        "id": "sFUJJmpInIP0"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fs = df[fs].values"
      ],
      "metadata": {
        "id": "0ktGMg_enJlU"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA0X5Tv6nKq-",
        "outputId": "b93ee898-d33c-448b-d9fe-6e45495fc758"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25490, 44)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature selection by Fast Correlation Based Filter (FCBF)\n"
      ],
      "metadata": {
        "id": "_Nq-i1gSnNoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUSgzfmKnNH0",
        "outputId": "3a45b0c1-bb7d-4bdb-fc16-f2824fd2fb92"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimizer in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.5.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (25.7.0)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimizer) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->scikit-optimizer) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import gp_minimize\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "def fcbf_objective(threshold, X, y, clf=RandomForestClassifier(random_state=42)):\n",
        "    selector = FCBF(th=threshold)\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "    if X_selected.shape[1] == 0:\n",
        "        return 1.0\n",
        "    score = cross_val_score(clf, X_selected, y, cv=3, scoring=\"accuracy\").mean()\n",
        "    return -score"
      ],
      "metadata": {
        "id": "P47o6mWfnQe1"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from FCBF_module import FCBF, FCBFK, FCBFiP, get_i\n",
        "\n",
        "res = gp_minimize(\n",
        "    lambda th: fcbf_objective(th[0], X_fs, y),\n",
        "    dimensions=[(0.01, 0.5)],\n",
        "    n_calls=20,\n",
        "    random_state=42,\n",
        "    acq_func='EI',\n",
        ")\n",
        "\n",
        "best_threshold = res.x[0]\n",
        "print(\"Best threshold:\", best_threshold)\n",
        "print(\"Best accuracy:\", -res.fun)"
      ],
      "metadata": {
        "id": "WoTwizkdnRoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f1a369-a345-4295-b61e-2ebef0cd38ec"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.09988304703442026\n",
            "Best accuracy: 0.8921943879088525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from FCBF_module import FCBF, FCBFK, FCBFiP, get_i\n",
        "fcbf = FCBF(th = best_threshold)"
      ],
      "metadata": {
        "id": "k7f6adG3nS3k"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fss = fcbf.fit_transform(X_fs,y)"
      ],
      "metadata": {
        "id": "pq-8md4PnT_E"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  kernel principal component analysis (KPCA)"
      ],
      "metadata": {
        "id": "TVZK6sdvnXlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Categorical\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def kpca_objective(params, X, y):\n",
        "    n_components, kernel = params\n",
        "    n_components = min(n_components, X.shape[1] - 1)\n",
        "    if n_components < 1: n_components = 1\n",
        "    try:\n",
        "        kpca = KernelPCA(n_components=n_components, kernel=kernel, fit_inverse_transform=False, random_state=42)\n",
        "        X_kpca = kpca.fit_transform(X)\n",
        "        clf = RandomForestClassifier(random_state=42)\n",
        "        score = cross_val_score(clf, X_kpca, y, cv=3, scoring=\"accuracy\").mean()\n",
        "        return -score\n",
        "    except Exception as e:\n",
        "        return 1.0\n",
        "\n",
        "search_space = [\n",
        "    Integer(2, 50),\n",
        "    Categorical(['rbf', 'poly'])\n",
        "]\n",
        "\n",
        "result = gp_minimize(\n",
        "    lambda params: kpca_objective(params, X_fss, y),\n",
        "    search_space,\n",
        "    n_calls=20,\n",
        "    random_state=42,\n",
        "    acq_func='EI'\n",
        ")\n",
        "\n",
        "best_n_components, best_kernel = result.x\n",
        "print(f\"Best n_components: {best_n_components}, Best kernel: {best_kernel}\")\n",
        "print(f\"Best accuracy: {-result.fun:.4f}\")"
      ],
      "metadata": {
        "id": "6T05jbegnVEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1706b8-39cf-44f9-92d4-0eba17500bed"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best n_components: 39, Best kernel: rbf\n",
            "Best accuracy: 0.8927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kpca = KernelPCA(n_components=best_n_components, kernel=best_kernel, random_state=42)\n",
        "X_kpca = kpca.fit_transform(X_fss)"
      ],
      "metadata": {
        "id": "-27MFOlbnZlm"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-test split after feature selection"
      ],
      "metadata": {
        "id": "I-YSMec6ncrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_idx = df.query('Label == 0').index\n",
        "df_val_test_idx = df.drop(df_train_idx).index"
      ],
      "metadata": {
        "id": "dGT0kP25Gy34"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_kpca[df_train_idx]\n",
        "y_train = np.zeros(len(X_train), dtype=int)"
      ],
      "metadata": {
        "id": "yBnFDUW2G0WY"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "1_WKlr5HG1UR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c6e5c4-4413-4557-b4fc-98001c52e5aa"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10226, 39)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "iI--UTy9G3yL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2884a2-f61a-4eb1-c26a-9884c27edadb"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10226,)"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_test_data = X_kpca[df_val_test_idx]\n",
        "val_test_labels = df.loc[df_val_test_idx, 'Label']"
      ],
      "metadata": {
        "id": "B9OF_bT3G6pZ"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_val_test_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY-DxhzlO_sN",
        "outputId": "62ec454c-df8f-45be-c164-f4f3e2b6328a"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15264"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w10GYnFZPnJg",
        "outputId": "6a1b3c8e-102c-44ac-e82e-eb8d0cef9a0c"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15264, 39)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_test_labels.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "4P4nJYkUkJHz",
        "outputId": "bf1cfd6b-d5c9-4d4b-b0bd-8146798acda7"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "1     13298\n",
              "15     1966\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1966</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if isinstance(X_val_test_data, (pd.Series, pd.DataFrame)):\n",
        "    X_val_test_data = X_val_test_data.to_numpy()\n",
        "\n",
        "if isinstance(val_test_labels, (pd.Series, pd.DataFrame)):\n",
        "    val_test_labels = val_test_labels.to_numpy()\n",
        "\n",
        "indices_1 = np.where(val_test_labels == 1)[0]\n",
        "indices_13 = np.where(val_test_labels == 15)[0]\n",
        "\n",
        "X_test_label_13 = X_val_test_data[indices_13]\n",
        "y_test_label_13 = val_test_labels[indices_13]\n",
        "\n",
        "num_label_13 = len(y_test_label_13)\n",
        "\n",
        "indices_0_train = np.where(y_train == 0)[0]\n",
        "num_label_0_train = len(indices_0_train)\n",
        "\n",
        "np.random.seed(42)\n",
        "if num_label_0_train >= num_label_13:\n",
        "    indices_0_test_sample = np.random.choice(indices_0_train, size=num_label_13, replace=False)\n",
        "else:\n",
        "    indices_0_test_sample = np.random.choice(indices_0_train, size=num_label_0_train, replace=False)\n",
        "\n",
        "X_test_label_0 = X_train[indices_0_test_sample]\n",
        "y_test_label_0 = y_train[indices_0_test_sample]\n",
        "\n",
        "X_test = np.concatenate([X_test_label_13, X_test_label_0], axis=0)\n",
        "y_test = np.concatenate([y_test_label_13, y_test_label_0], axis=0)\n",
        "\n",
        "X_val_label_1 = X_val_test_data[indices_1]\n",
        "y_val_label_1 = val_test_labels[indices_1]\n",
        "\n",
        "remaining_indices_0_train = np.setdiff1d(indices_0_train, indices_0_test_sample)\n",
        "\n",
        "X_val_label_0 = X_train[remaining_indices_0_train]\n",
        "y_val_label_0 = y_train[remaining_indices_0_train]\n",
        "\n",
        "num_val_label_0 = len(y_val_label_0)\n",
        "num_val_label_1 = len(y_val_label_1)\n",
        "\n",
        "np.random.seed(42)\n",
        "if num_val_label_1 >= num_val_label_0:\n",
        "    indices_1_val_sample = np.random.choice(num_val_label_1, size=num_val_label_0, replace=False)\n",
        "    X_val_label_1_final = X_val_label_1[indices_1_val_sample]\n",
        "    y_val_label_1_final = y_val_label_1[indices_1_val_sample]\n",
        "else:\n",
        "    X_val_label_1_final = X_val_label_1\n",
        "    y_val_label_1_final = y_val_label_1\n",
        "\n",
        "X_val = np.concatenate([X_val_label_0, X_val_label_1_final], axis=0)\n",
        "y_val = np.concatenate([y_val_label_0, y_val_label_1_final], axis=0)\n",
        "\n",
        "print(\"Shape of final X_val:\", X_val.shape)\n",
        "print(\"Shape of final y_val:\", y_val.shape)\n",
        "unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
        "print(\"y_val label counts:\", dict(zip(unique_val, counts_val)))\n",
        "\n",
        "print(\"\\nShape of final X_test:\", X_test.shape)\n",
        "print(\"Shape of final y_test:\", y_test.shape)\n",
        "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "print(\"y_test label counts:\", dict(zip(unique_test, counts_test)))"
      ],
      "metadata": {
        "id": "EkI3DgTzRw_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327ca40b-d73a-4172-98dc-710f92eb3bb1"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of final X_val: (16520, 39)\n",
            "Shape of final y_val: (16520,)\n",
            "y_val label counts: {np.int64(0): np.int64(8260), np.int64(1): np.int64(8260)}\n",
            "\n",
            "Shape of final X_test: (3932, 39)\n",
            "Shape of final y_test: (3932,)\n",
            "y_test label counts: {np.int64(0): np.int64(1966), np.int64(15): np.int64(1966)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val[y_val != 0] = 1\n",
        "\n",
        "y_test[y_test != 0] = 1\n"
      ],
      "metadata": {
        "id": "-8O3c8sNmuCF"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R35HcAwJnHO",
        "outputId": "1de4d971-191d-4e69-91ab-1915f19d346f"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16520, 39)"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "std_scaler = MinMaxScaler()\n",
        "std_scaler = std_scaler.fit(X_train)\n",
        "\n",
        "X_train = std_scaler.transform(X_train)\n",
        "X_val = std_scaler.transform(X_val)\n",
        "X_test = std_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "QvUcy_l_cLGa"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply the Autoencoder model with biased classifiers"
      ],
      "metadata": {
        "id": "fEg4XjxKobsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementação do Early Stopping\n",
        "class EarlyStopping:\n",
        "  def __init__(self, patience=7, delta=0, verbose=True, path='checkpoint.pt'):\n",
        "      self.patience = patience\n",
        "      self.delta = delta\n",
        "      self.verbose = verbose\n",
        "      self.counter = 0\n",
        "      self.early_stop = False\n",
        "      self.val_min_loss = np.inf\n",
        "      self.path = path\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "    if val_loss < self.val_min_loss - self.delta:   # Caso a loss da validação reduza, vamos salvar o modelo e nova loss mínima\n",
        "      self.save_checkpoint(val_loss, model)\n",
        "      self.counter = 0\n",
        "    else:                                           # Caso a loss da validação NÃO reduza, vamos incrementar o contador da paciencia\n",
        "      self.counter += 1\n",
        "      print(f'EarlyStopping counter: {self.counter} out of {self.patience}. Current validation loss: {val_loss:.5f}')\n",
        "      if self.counter >= self.patience:\n",
        "          self.early_stop = True\n",
        "\n",
        "  def save_checkpoint(self, val_loss, model):\n",
        "    if self.verbose:\n",
        "        print(f'Validation loss decreased ({self.val_min_loss:.5f} --> {val_loss:.5f}).  Saving model ...')\n",
        "    torch.save(model.state_dict(), self.path)\n",
        "    self.val_min_loss = val_loss"
      ],
      "metadata": {
        "id": "0nAxBs0PoZul"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, in_features, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.early_stopping = None\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_features, 24),\n",
        "            nn.BatchNorm1d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(24, 8),\n",
        "            nn.BatchNorm1d(8),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(8, 24),\n",
        "            nn.BatchNorm1d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(24, in_features),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        encoded = self.encoder(X)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        "\n",
        "    def compile(self, learning_rate):\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def fit_transform(self, X_train, num_epochs, batch_size,\n",
        "                      X_val=None, patience=None, delta=None, device=None):\n",
        "        \"\"\"\n",
        "        Trains the autoencoder and returns the encoded representation of X_train.\n",
        "        \"\"\"\n",
        "        if X_val is not None and patience is not None and delta is not None:\n",
        "            print(f'Using early stopping with patience={patience} and delta={delta}')\n",
        "            self.early_stopping = EarlyStopping(patience, delta)\n",
        "\n",
        "        val_avg_losses = []\n",
        "        train_avg_losses = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_losses = []\n",
        "            self.train()\n",
        "            for batch in range(0, len(X_train), batch_size):\n",
        "                batch_X = X_train[batch:(batch + batch_size)]\n",
        "                encoded, decoded = self.forward(batch_X)\n",
        "\n",
        "                loss = self.criterion(decoded, batch_X)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "            train_avg_loss = np.mean(train_losses)\n",
        "            train_avg_losses.append(train_avg_loss)\n",
        "            print(f'Epoch#{epoch+1}: Train Average Loss = {train_avg_loss:.5f}')\n",
        "\n",
        "            if self.early_stopping is not None:\n",
        "                val_losses = []\n",
        "                self.eval()\n",
        "                with torch.no_grad():\n",
        "                    for batch in range(0, len(X_val), batch_size):\n",
        "                        batch_X = X_val[batch:(batch + batch_size)]\n",
        "                        _, decoded = self.forward(batch_X)\n",
        "                        val_loss = self.criterion(decoded, batch_X)\n",
        "                        val_losses.append(val_loss.item())\n",
        "                val_avg_loss = np.mean(val_losses)\n",
        "                val_avg_losses.append(val_avg_loss)\n",
        "                self.early_stopping(val_avg_loss, self)\n",
        "                if self.early_stopping.early_stop:\n",
        "                    print(f'Stopped by early stopping at epoch {epoch+1}')\n",
        "                    break\n",
        "\n",
        "        if self.early_stopping is not None:\n",
        "            self.load_state_dict(torch.load('checkpoint.pt', map_location=device))\n",
        "\n",
        "        self.eval()\n",
        "        return self.transform(X_train, batch_size)\n",
        "\n",
        "    def transform(self, X, batch_size=256):\n",
        "        \"\"\"\n",
        "        Returns the encoded representation of the input X.\n",
        "        \"\"\"\n",
        "        encoded_list = []\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in range(0, len(X), batch_size):\n",
        "                batch_X = X[batch:(batch + batch_size)]\n",
        "                encoded, _ = self.forward(batch_X)\n",
        "                encoded_list.append(encoded.cpu())\n",
        "        return torch.cat(encoded_list, dim=0)\n"
      ],
      "metadata": {
        "id": "DPejKYPIofpF"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer, Real\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class SklearnAutoencoder(BaseEstimator):\n",
        "    def __init__(self, in_features, hidden1=24, hidden2=8, dropout=0.2, lr=1e-3, num_epochs=50, batch_size=256):\n",
        "        self.in_features = in_features\n",
        "        self.hidden1 = hidden1\n",
        "        self.hidden2 = hidden2\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        self.model = Autoencoder(self.in_features, dropout_rate=self.dropout)\n",
        "        self.model.encoder = nn.Sequential(\n",
        "            nn.Linear(self.in_features, self.hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden1, self.hidden2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.model.decoder = nn.Sequential(\n",
        "            nn.Linear(self.hidden2, self.hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden1, self.in_features),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.model.to(self.device)\n",
        "        self.model.compile(self.lr)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        _ = self.model.fit_transform(X_tensor, self.num_epochs, self.batch_size)\n",
        "        return self\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        _, decoded = self.model(X_tensor)\n",
        "        decoded = decoded.detach().cpu().numpy()\n",
        "        loss = np.mean((X - decoded) ** 2)\n",
        "        return -loss\n"
      ],
      "metadata": {
        "id": "Zh8TShj7oh9c"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "def ae_scorer(estimator, X, y=None):\n",
        "    return estimator.score(X)\n",
        "\n",
        "search_space = {\n",
        "    \"hidden1\": Integer(32, 256),   # bigger first hidden layer\n",
        "    \"hidden2\": Integer(8, 128),    # bigger bottleneck\n",
        "    \"dropout\": Real(0.0, 0.5),\n",
        "    \"lr\": Real(1e-4, 5e-3, \"log-uniform\")\n",
        "}\n",
        "\n",
        "opt_ae = BayesSearchCV(\n",
        "    SklearnAutoencoder(in_features=X_train.shape[1]),\n",
        "    search_space,\n",
        "    n_iter=50,\n",
        "    cv=10,\n",
        "    n_jobs=-1,\n",
        "    scoring=ae_scorer,   # use the custom scorer\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "opt_ae.fit(X_train)\n",
        "best_ae = opt_ae.best_estimator_.model\n"
      ],
      "metadata": {
        "id": "Yt8egvrvokFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39729893-b6de-4705-f7f5-05fb23d9b2bc"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch#1: Train Average Loss = 0.14062\n",
            "Epoch#2: Train Average Loss = 0.07624\n",
            "Epoch#3: Train Average Loss = 0.01729\n",
            "Epoch#4: Train Average Loss = 0.01086\n",
            "Epoch#5: Train Average Loss = 0.00918\n",
            "Epoch#6: Train Average Loss = 0.00823\n",
            "Epoch#7: Train Average Loss = 0.00760\n",
            "Epoch#8: Train Average Loss = 0.00726\n",
            "Epoch#9: Train Average Loss = 0.00704\n",
            "Epoch#10: Train Average Loss = 0.00686\n",
            "Epoch#11: Train Average Loss = 0.00678\n",
            "Epoch#12: Train Average Loss = 0.00666\n",
            "Epoch#13: Train Average Loss = 0.00660\n",
            "Epoch#14: Train Average Loss = 0.00651\n",
            "Epoch#15: Train Average Loss = 0.00646\n",
            "Epoch#16: Train Average Loss = 0.00638\n",
            "Epoch#17: Train Average Loss = 0.00630\n",
            "Epoch#18: Train Average Loss = 0.00611\n",
            "Epoch#19: Train Average Loss = 0.00575\n",
            "Epoch#20: Train Average Loss = 0.00507\n",
            "Epoch#21: Train Average Loss = 0.00426\n",
            "Epoch#22: Train Average Loss = 0.00377\n",
            "Epoch#23: Train Average Loss = 0.00355\n",
            "Epoch#24: Train Average Loss = 0.00343\n",
            "Epoch#25: Train Average Loss = 0.00338\n",
            "Epoch#26: Train Average Loss = 0.00333\n",
            "Epoch#27: Train Average Loss = 0.00332\n",
            "Epoch#28: Train Average Loss = 0.00325\n",
            "Epoch#29: Train Average Loss = 0.00326\n",
            "Epoch#30: Train Average Loss = 0.00321\n",
            "Epoch#31: Train Average Loss = 0.00318\n",
            "Epoch#32: Train Average Loss = 0.00317\n",
            "Epoch#33: Train Average Loss = 0.00316\n",
            "Epoch#34: Train Average Loss = 0.00312\n",
            "Epoch#35: Train Average Loss = 0.00311\n",
            "Epoch#36: Train Average Loss = 0.00308\n",
            "Epoch#37: Train Average Loss = 0.00306\n",
            "Epoch#38: Train Average Loss = 0.00306\n",
            "Epoch#39: Train Average Loss = 0.00305\n",
            "Epoch#40: Train Average Loss = 0.00303\n",
            "Epoch#41: Train Average Loss = 0.00303\n",
            "Epoch#42: Train Average Loss = 0.00301\n",
            "Epoch#43: Train Average Loss = 0.00299\n",
            "Epoch#44: Train Average Loss = 0.00297\n",
            "Epoch#45: Train Average Loss = 0.00298\n",
            "Epoch#46: Train Average Loss = 0.00295\n",
            "Epoch#47: Train Average Loss = 0.00295\n",
            "Epoch#48: Train Average Loss = 0.00292\n",
            "Epoch#49: Train Average Loss = 0.00290\n",
            "Epoch#50: Train Average Loss = 0.00291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Y4uFg3M1nGe0"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_overall_metrics(y_true, y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "  acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "  tpr = tp/(tp+fn)\n",
        "  fpr = fp/(fp+tn)\n",
        "  precision = tp/(tp+fp)\n",
        "  f1 = (2*tpr*precision)/(tpr+precision)\n",
        "  return {'acc':acc,'tpr':tpr,'fpr':fpr,'precision':precision,'f1-score':f1}"
      ],
      "metadata": {
        "id": "ifNLmQFrne_0"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    recall_score\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "def Anomaly_IDS(best_ae, input_dim, X_train, X_test, y_train, y_test, X_val, y_val, n, uncertainty_margin=0.01,b=100, num_epochs=50, batch_size=256):\n",
        "\n",
        "    ae = best_ae\n",
        "    X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "    _ = ae.fit_transform(X_train_torch, num_epochs, batch_size)\n",
        "\n",
        "    def get_autoencoder_anomaly_scores(model, X):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_torch = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "            # The model returns (latent_representation, reconstruction)\n",
        "            _, reconstructed_X = model(X_torch)\n",
        "            # Calculate Mean Squared Error for each sample\n",
        "            anomaly_scores = torch.mean(torch.pow(X_torch - reconstructed_X, 2), axis=1)\n",
        "            return anomaly_scores.detach().cpu().numpy()\n",
        "\n",
        "    val_anomaly_scores = get_autoencoder_anomaly_scores(ae, X_val)\n",
        "    fpr, tpr, thresholds = roc_curve(y_val, val_anomaly_scores)\n",
        "\n",
        "    print('Fpr', fpr)\n",
        "    print('Tpr', tpr)\n",
        "    print('Thresholds', thresholds)\n",
        "\n",
        "    df_roc = pd.DataFrame({\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'thresholds': thresholds\n",
        "    })\n",
        "\n",
        "    df_roc['youden_index'] = df_roc['tpr'] - df_roc['fpr']\n",
        "    best_row = df_roc.loc[df_roc['youden_index'].idxmax()]\n",
        "    optimal_threshold = best_row['thresholds']\n",
        "\n",
        "    test_anomaly_scores = get_autoencoder_anomaly_scores(ae, X_test)\n",
        "\n",
        "    y_pred_ae = (test_anomaly_scores >= optimal_threshold).astype(int)\n",
        "\n",
        "    print(\"Autoencoder Performance (threshold-based):\")\n",
        "    print(classification_report(y_test, y_pred_ae))\n",
        "    cm_ae = confusion_matrix(y_test, y_pred_ae)\n",
        "    tn, fp, fn, tp = cm_ae.ravel()\n",
        "    dr_ae = tp/(tp+fn) if tp+fn>0 else 0\n",
        "    far_ae = fp/(fp+tn) if fp+tn>0 else 0\n",
        "    acc_ae = accuracy_score(y_test, y_pred_ae)\n",
        "    print(f\"  Acc: {acc_ae:.4f}, DR: {dr_ae:.4f}, FAR: {far_ae:.4f}\\n  CM:\\n{cm_ae}\")\n",
        "\n",
        "    val_anomaly_scores = get_autoencoder_anomaly_scores(ae, X_val)\n",
        "\n",
        "    # 1. Identify False Positives (FP) and False Negatives (FN) from the training set\n",
        "    fp_idx = np.where((val_anomaly_scores >= optimal_threshold) & (y_val == 0))[0]\n",
        "    fn_idx = np.where((val_anomaly_scores < optimal_threshold) & (y_val == 1))[0]\n",
        "\n",
        "    X_fp_train = X_val[fp_idx] # Normal samples misclassified as Attack\n",
        "    X_fn_train = X_val[fn_idx] # Attack samples misclassified as Normal\n",
        "\n",
        "    rfp = None\n",
        "    rfn = None\n",
        "    cv_splits = 5\n",
        "\n",
        "    if len(X_fp_train) > 0:\n",
        "        X_attacks_for_fp = X_train[y_train == 1]\n",
        "        if len(X_attacks_for_fp) > 0:\n",
        "            attack_samples_for_fp = X_attacks_for_fp[np.random.choice(len(X_attacks_for_fp), size=len(X_fp_train), replace=True)]\n",
        "            Xp = np.concatenate([X_fp_train, attack_samples_for_fp])\n",
        "            yp = np.concatenate([np.zeros(len(X_fp_train)), np.ones(len(attack_samples_for_fp))])\n",
        "            if min(np.bincount(yp.astype(int))) >= cv_splits:\n",
        "                # Use BayesSearchCV if enough samples exist\n",
        "                opt_rfp = BayesSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': Integer(10, 200), 'max_depth': Integer(3, 50), 'min_samples_split': Integer(2, 10)}, n_iter=20, cv=StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42), scoring='f1', n_jobs=-1, random_state=42)\n",
        "                opt_rfp.fit(Xp, yp)\n",
        "                rfp = opt_rfp.best_estimator_\n",
        "            else:\n",
        "                rfp = RandomForestClassifier(random_state=42).fit(Xp, yp)\n",
        "\n",
        "    if len(X_fn_train) > 0:\n",
        "        X_normals_for_fn = X_train[y_train == 0]\n",
        "        if len(X_normals_for_fn) > 0:\n",
        "            normal_samples_for_fn = X_normals_for_fn[np.random.choice(len(X_normals_for_fn), size=len(X_fn_train), replace=True)]\n",
        "            Xn = np.concatenate([X_fn_train, normal_samples_for_fn])\n",
        "            yn = np.concatenate([np.ones(len(X_fn_train)), np.zeros(len(normal_samples_for_fn))])\n",
        "            if min(np.bincount(yn.astype(int))) >= cv_splits:\n",
        "                # Use BayesSearchCV if enough samples exist\n",
        "                opt_rfn = BayesSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': Integer(10, 200), 'max_depth': Integer(3, 50), 'min_samples_split': Integer(2, 10)}, n_iter=20, cv=StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42), scoring='f1', n_jobs=-1, random_state=42)\n",
        "                opt_rfn.fit(Xn, yn)\n",
        "                rfn = opt_rfn.best_estimator_\n",
        "            else:\n",
        "                rfn = RandomForestClassifier(random_state=42).fit(Xn, yn)\n",
        "\n",
        "\n",
        "    y_final = y_pred_ae.copy()\n",
        "    print(\"\\n--- Applying Targeted Correction ---\")\n",
        "\n",
        "    if rfn is not None:\n",
        "        rfn_check_indices = np.where(\n",
        "            (y_pred_ae == 0) &\n",
        "            (test_anomaly_scores > optimal_threshold - uncertainty_margin)\n",
        "        )[0]\n",
        "        print(f\"Found {len(rfn_check_indices)} uncertain 'Normal' samples to re-classify with RFN.\")\n",
        "        if len(rfn_check_indices) > 0:\n",
        "            rfn_predictions = rfn.predict(X_test[rfn_check_indices])\n",
        "            y_final[rfn_check_indices] = rfn_predictions\n",
        "\n",
        "    if rfp is not None:\n",
        "        rfp_check_indices = np.where(\n",
        "            (y_pred_ae == 1) &\n",
        "            (test_anomaly_scores < optimal_threshold + uncertainty_margin)\n",
        "        )[0]\n",
        "        print(f\"Found {len(rfp_check_indices)} uncertain 'Attack' samples to re-classify with RFP.\")\n",
        "        if len(rfp_check_indices) > 0:\n",
        "            rfp_predictions = rfp.predict(X_test[rfp_check_indices])\n",
        "            y_final[rfp_check_indices] = rfp_predictions\n",
        "\n",
        "    print(classification_report(y_test, y_final, target_names=['Normal', 'Attack']))\n",
        "    cm_final = confusion_matrix(y_test, y_final)\n",
        "\n",
        "    tn_f, fp_f, fn_f, tp_f = cm_final.ravel()\n",
        "    acc_final = accuracy_score(y_test, y_final)\n",
        "    dr_final = tp_f / (tp_f + fn_f) if (tp_f + fn_f) > 0 else 0\n",
        "    far_final = fp_f / (fp_f + tn_f) if (fp_f + tn_f) > 0 else 0\n",
        "\n",
        "    print(f\"  Acc: {acc_final:.4f}, DR: {dr_final:.4f}, FAR: {far_final:.4f}\\n  CM:\\n{cm_final}\")\n",
        "\n",
        "    return acc_final, dr_final, far_final, cm_final\n",
        "\n"
      ],
      "metadata": {
        "id": "0ffL9boeohDl"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_val).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "QYZ0nb-2M8_a",
        "outputId": "3039ba55-b261-4501-8a41-87e1c66fbf3a"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    8260\n",
              "1    8260\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8260</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score, recall_score, roc_curve\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def Anomaly_IDS(best_ae, X_train, y_train, X_test, y_test, X_val, y_val, num_epochs=50, batch_size=256):\n",
        "    X_train_benign = X_train[y_train == 0]\n",
        "    X_train_benign_torch = torch.tensor(X_train_benign, dtype=torch.float32)\n",
        "    ae = best_ae\n",
        "    _ = ae.fit_transform(X_train_benign_torch, num_epochs, batch_size)\n",
        "\n",
        "    def get_autoencoder_anomaly_scores(model, X):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_torch = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "            _, reconstructed_X = model(X_torch)\n",
        "            anomaly_scores = torch.mean(torch.pow(X_torch - reconstructed_X, 2), axis=1)\n",
        "            return anomaly_scores.detach().cpu().numpy()\n",
        "\n",
        "    X_val_train, X_val_tune, y_val_train, y_val_tune = train_test_split(\n",
        "        X_val, y_val, test_size=0.4, random_state=42, stratify=y_val\n",
        "    )\n",
        "\n",
        "    val_train_scores = get_autoencoder_anomaly_scores(ae, X_val_train)\n",
        "    fpr, tpr, thresholds = roc_curve(y_val_train, val_train_scores)\n",
        "\n",
        "    df_roc = pd.DataFrame({'tpr': tpr, 'fpr': fpr, 'thresholds': thresholds})\n",
        "    df_roc['youden_index'] = df_roc['tpr'] - df_roc['fpr']\n",
        "    # Handle cases where max index might not be unique or thresholds are infinite\n",
        "    df_roc.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_roc.dropna(inplace=True)\n",
        "    optimal_threshold = df_roc.loc[df_roc['youden_index'].idxmax()]['thresholds']\n",
        "\n",
        "    fp_idx = np.where((val_train_scores >= optimal_threshold) & (y_val_train == 0))[0]\n",
        "    fn_idx = np.where((val_train_scores < optimal_threshold) & (y_val_train == 1))[0]\n",
        "\n",
        "    X_fp_train = X_val_train[fp_idx]\n",
        "    X_fn_train = X_val_train[fn_idx]\n",
        "\n",
        "    # (The logic for training rfp and rfn remains identical, just using the new data)\n",
        "    rfp, rfn = train_specialist_classifiers(X_fp_train, X_fn_train, X_train, y_train)\n",
        "\n",
        "    # --- Step 2: Tune the uncertainty_margin on the val_tune set ---\n",
        "    print(\"\\nTuning the uncertainty margin...\")\n",
        "    # Define candidate margins as a percentage of the optimal threshold\n",
        "    margin_candidates_relative = np.linspace(0.01, 0.50, 10) # e.g., 1% to 50%\n",
        "    margin_candidates = [relative * optimal_threshold for relative in margin_candidates_relative]\n",
        "\n",
        "    best_margin = 0\n",
        "    best_f1 = -1\n",
        "\n",
        "    val_tune_scores = get_autoencoder_anomaly_scores(ae, X_val_tune)\n",
        "    y_pred_ae_tune = (val_tune_scores >= optimal_threshold).astype(int)\n",
        "\n",
        "    for margin in margin_candidates:\n",
        "        y_final_tune = apply_correction(\n",
        "            y_pred_ae_tune, val_tune_scores, X_val_tune, rfp, rfn, optimal_threshold, margin\n",
        "        )\n",
        "        current_f1 = f1_score(y_val_tune, y_final_tune)\n",
        "\n",
        "        if current_f1 > best_f1:\n",
        "            best_f1 = current_f1\n",
        "            best_margin = margin\n",
        "\n",
        "    print(f\"Optimal uncertainty margin found: {best_margin:.6f} (Resulting F1 on tune set: {best_f1:.4f})\")\n",
        "\n",
        "    # --- Step 3: Final evaluation on the unseen test set using the best margin ---\n",
        "    print(\"\\n--- Final Evaluation on Test Set ---\")\n",
        "    test_anomaly_scores = get_autoencoder_anomaly_scores(ae, X_test)\n",
        "    y_pred_ae_test = (test_anomaly_scores >= optimal_threshold).astype(int)\n",
        "\n",
        "    print(\"\\nAutoencoder Performance (baseline):\")\n",
        "    print_metrics(y_test, y_pred_ae_test)\n",
        "\n",
        "    print(\"\\n--- Applying Targeted Correction with Tuned Margin ---\")\n",
        "    y_final_test = apply_correction(\n",
        "        y_pred_ae_test, test_anomaly_scores, X_test, rfp, rfn, optimal_threshold, best_margin\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal Hybrid Model Performance:\")\n",
        "    tn_f, fp_f, fn_f, tp_f, acc_final, dr_final, far_final, cm_final = print_metrics(y_test, y_final_test, return_raw=True)\n",
        "\n",
        "    return acc_final, dr_final, far_final, cm_final\n",
        "\n",
        "# --- Helper function to avoid code repetition ---\n",
        "def train_specialist_classifiers(X_fp_train, X_fn_train, X_train, y_train):\n",
        "    rfp = None\n",
        "    rfn = None\n",
        "    cv_splits = 5\n",
        "\n",
        "    # RFP Training Logic (moved here from the main function)\n",
        "    if len(X_fp_train) > 0:\n",
        "        X_attacks_for_fp = X_train[y_train == 1]\n",
        "        if len(X_attacks_for_fp) > 0:\n",
        "            attack_samples_for_fp = X_attacks_for_fp[np.random.choice(len(X_attacks_for_fp), size=len(X_fp_train), replace=True)]\n",
        "            Xp = np.concatenate([X_fp_train, attack_samples_for_fp])\n",
        "            yp = np.concatenate([np.zeros(len(X_fp_train)), np.ones(len(attack_samples_for_fp))])\n",
        "            if min(np.bincount(yp.astype(int))) >= cv_splits:\n",
        "                opt_rfp = BayesSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': Integer(10, 200), 'max_depth': Integer(3, 50), 'min_samples_split': Integer(2, 10)}, n_iter=20, cv=StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42), scoring='f1', n_jobs=-1, random_state=42)\n",
        "                opt_rfp.fit(Xp, yp)\n",
        "                rfp = opt_rfp.best_estimator_\n",
        "            else:\n",
        "                rfp = RandomForestClassifier(random_state=42).fit(Xp, yp)\n",
        "\n",
        "    # RFN Training Logic (moved here from the main function)\n",
        "    if len(X_fn_train) > 0:\n",
        "        X_normals_for_fn = X_train[y_train == 0]\n",
        "        if len(X_normals_for_fn) > 0:\n",
        "            normal_samples_for_fn = X_normals_for_fn[np.random.choice(len(X_normals_for_fn), size=len(X_fn_train), replace=True)]\n",
        "            Xn = np.concatenate([X_fn_train, normal_samples_for_fn])\n",
        "            yn = np.concatenate([np.ones(len(X_fn_train)), np.zeros(len(normal_samples_for_fn))])\n",
        "            if min(np.bincount(yn.astype(int))) >= cv_splits:\n",
        "                opt_rfn = BayesSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': Integer(10, 200), 'max_depth': Integer(3, 50), 'min_samples_split': Integer(2, 10)}, n_iter=20, cv=StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42), scoring='f1', n_jobs=-1, random_state=42)\n",
        "                opt_rfn.fit(Xn, yn)\n",
        "                rfn = opt_rfn.best_estimator_\n",
        "            else:\n",
        "                rfn = RandomForestClassifier(random_state=42).fit(Xn, yn)\n",
        "\n",
        "    return rfp, rfn\n",
        "\n",
        "def apply_correction(y_pred_initial, anomaly_scores, X_data, rfp, rfn, threshold, margin):\n",
        "    y_final = y_pred_initial.copy()\n",
        "\n",
        "    # Apply RFN to uncertain 'Normal' samples\n",
        "    if rfn is not None:\n",
        "        rfn_indices = np.where((y_pred_initial == 0) & (anomaly_scores > threshold - margin))[0]\n",
        "        if len(rfn_indices) > 0:\n",
        "            rfn_preds = rfn.predict(X_data[rfn_indices])\n",
        "            y_final[rfn_indices] = rfn_preds\n",
        "\n",
        "    # Apply RFP to uncertain 'Attack' samples\n",
        "    if rfp is not None:\n",
        "        rfp_indices = np.where((y_pred_initial == 1) & (anomaly_scores < threshold + margin))[0]\n",
        "        if len(rfp_indices) > 0:\n",
        "            rfp_preds = rfp.predict(X_data[rfp_indices])\n",
        "            y_final[rfp_indices] = rfp_preds\n",
        "\n",
        "    return y_final\n",
        "\n",
        "def print_metrics(y_true, y_pred, return_raw=False):\n",
        "    print(classification_report(y_true, y_pred, target_names=['Normal', 'Attack']))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    dr = recall_score(y_true, y_pred) # Detection Rate is just recall\n",
        "    far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    print(f\"  Acc: {acc:.4f}, DR: {dr:.4f}, FAR: {far:.4f}\\n  CM:\\n{cm}\")\n",
        "    if return_raw:\n",
        "        return tn, fp, fn, tp, acc, dr, far, cm"
      ],
      "metadata": {
        "id": "QDcqQAGn17yM"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Anomaly_IDS(\n",
        "    best_ae=best_ae,\n",
        "    X_train=X_train,   # convert to numpy\n",
        "    X_test=X_test,\n",
        "    y_train=y_train,   # convert to numpy\n",
        "    y_test=y_test,\n",
        "    X_val=X_val,\n",
        "    y_val=y_val\n",
        ")"
      ],
      "metadata": {
        "id": "X64donH9olfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc1ecc3-1e15-4c04-9f4b-17f29bfc643b"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch#1: Train Average Loss = 0.00287\n",
            "Epoch#2: Train Average Loss = 0.00287\n",
            "Epoch#3: Train Average Loss = 0.00285\n",
            "Epoch#4: Train Average Loss = 0.00284\n",
            "Epoch#5: Train Average Loss = 0.00283\n",
            "Epoch#6: Train Average Loss = 0.00282\n",
            "Epoch#7: Train Average Loss = 0.00279\n",
            "Epoch#8: Train Average Loss = 0.00277\n",
            "Epoch#9: Train Average Loss = 0.00277\n",
            "Epoch#10: Train Average Loss = 0.00275\n",
            "Epoch#11: Train Average Loss = 0.00274\n",
            "Epoch#12: Train Average Loss = 0.00273\n",
            "Epoch#13: Train Average Loss = 0.00271\n",
            "Epoch#14: Train Average Loss = 0.00271\n",
            "Epoch#15: Train Average Loss = 0.00269\n",
            "Epoch#16: Train Average Loss = 0.00268\n",
            "Epoch#17: Train Average Loss = 0.00265\n",
            "Epoch#18: Train Average Loss = 0.00266\n",
            "Epoch#19: Train Average Loss = 0.00263\n",
            "Epoch#20: Train Average Loss = 0.00263\n",
            "Epoch#21: Train Average Loss = 0.00260\n",
            "Epoch#22: Train Average Loss = 0.00258\n",
            "Epoch#23: Train Average Loss = 0.00259\n",
            "Epoch#24: Train Average Loss = 0.00257\n",
            "Epoch#25: Train Average Loss = 0.00256\n",
            "Epoch#26: Train Average Loss = 0.00254\n",
            "Epoch#27: Train Average Loss = 0.00254\n",
            "Epoch#28: Train Average Loss = 0.00252\n",
            "Epoch#29: Train Average Loss = 0.00251\n",
            "Epoch#30: Train Average Loss = 0.00250\n",
            "Epoch#31: Train Average Loss = 0.00247\n",
            "Epoch#32: Train Average Loss = 0.00248\n",
            "Epoch#33: Train Average Loss = 0.00246\n",
            "Epoch#34: Train Average Loss = 0.00246\n",
            "Epoch#35: Train Average Loss = 0.00245\n",
            "Epoch#36: Train Average Loss = 0.00243\n",
            "Epoch#37: Train Average Loss = 0.00241\n",
            "Epoch#38: Train Average Loss = 0.00243\n",
            "Epoch#39: Train Average Loss = 0.00243\n",
            "Epoch#40: Train Average Loss = 0.00239\n",
            "Epoch#41: Train Average Loss = 0.00239\n",
            "Epoch#42: Train Average Loss = 0.00238\n",
            "Epoch#43: Train Average Loss = 0.00238\n",
            "Epoch#44: Train Average Loss = 0.00236\n",
            "Epoch#45: Train Average Loss = 0.00238\n",
            "Epoch#46: Train Average Loss = 0.00236\n",
            "Epoch#47: Train Average Loss = 0.00234\n",
            "Epoch#48: Train Average Loss = 0.00233\n",
            "Epoch#49: Train Average Loss = 0.00234\n",
            "Epoch#50: Train Average Loss = 0.00233\n",
            "\n",
            "Tuning the uncertainty margin...\n",
            "Optimal uncertainty margin found: 0.000004 (Resulting F1 on tune set: 0.6916)\n",
            "\n",
            "--- Final Evaluation on Test Set ---\n",
            "\n",
            "Autoencoder Performance (baseline):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      0.11      0.21      1966\n",
            "      Attack       0.53      1.00      0.69      1966\n",
            "\n",
            "    accuracy                           0.56      3932\n",
            "   macro avg       0.77      0.56      0.45      3932\n",
            "weighted avg       0.77      0.56      0.45      3932\n",
            "\n",
            "  Acc: 0.5575, DR: 1.0000, FAR: 0.8850\n",
            "  CM:\n",
            "[[ 226 1740]\n",
            " [   0 1966]]\n",
            "\n",
            "--- Applying Targeted Correction with Tuned Margin ---\n",
            "\n",
            "Final Hybrid Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      0.11      0.21      1966\n",
            "      Attack       0.53      1.00      0.69      1966\n",
            "\n",
            "    accuracy                           0.56      3932\n",
            "   macro avg       0.77      0.56      0.45      3932\n",
            "weighted avg       0.77      0.56      0.45      3932\n",
            "\n",
            "  Acc: 0.5575, DR: 1.0000, FAR: 0.8850\n",
            "  CM:\n",
            "[[ 226 1740]\n",
            " [   0 1966]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5574771108850458,\n",
              " 1.0,\n",
              " np.float64(0.8850457782299085),\n",
              " array([[ 226, 1740],\n",
              "        [   0, 1966]]))"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    }
  ]
}