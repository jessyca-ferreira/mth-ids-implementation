{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "mj_TCXz1pRZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo_Hi9Q7or51",
        "outputId": "c2924394-0035-449b-e1af-1ae488676372"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimizer in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.5.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (25.7.0)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimizer) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->scikit-optimizer) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "hUFr_cMRmST7"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score,roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "9x_fFBnfmo9u"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train test split"
      ],
      "metadata": {
        "id": "-RgYN5zepWWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('./CICIDS2017_sample_km.csv')"
      ],
      "metadata": {
        "id": "c-HlLSyxmpXj"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Label.value_counts()"
      ],
      "metadata": {
        "id": "KfAU5Aq2mrVt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "9e8de989-0327-42b8-ebc2-bfeb3a8b407d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "0     10226\n",
              "4      4619\n",
              "10     3178\n",
              "2      2564\n",
              "1      1966\n",
              "12     1507\n",
              "14      652\n",
              "3       208\n",
              "7       155\n",
              "6       118\n",
              "11      116\n",
              "5       113\n",
              "9        36\n",
              "13       21\n",
              "8        11\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = (df['Label'] != 0) & (df['Label'] != 6)\n",
        "df.loc[mask, 'Label'] = 1\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "oeQhLxDw3u5X"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Label.value_counts()"
      ],
      "metadata": {
        "id": "M-Ne5x6031Dx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "531c3fa4-7c6e-42c1-dd52-9cba9bdcab4c"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "1    15146\n",
              "0    10226\n",
              "6      118\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop(['Label'],axis=1).dtypes[df.dtypes != 'object'].index\n",
        "df[features] = df[features].apply(\n",
        "    lambda x: (x - x.mean()) / (x.std()))\n",
        "\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df = df.fillna(0)"
      ],
      "metadata": {
        "id": "VcMt7ZWGmr0N"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Label', axis='columns')\n",
        "y = df['Label']\n",
        "y=np.ravel(y)\n",
        "pd.Series(y).value_counts()"
      ],
      "metadata": {
        "id": "mERlNXaLnE2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "70402aec-aa60-4c01-9b98-a68b5a0e9820"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    15146\n",
              "0    10226\n",
              "6      118\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering (IG, FCBF, and KPCA)"
      ],
      "metadata": {
        "id": "-csDS2vWmypc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "importances = mutual_info_classif(X, y)"
      ],
      "metadata": {
        "id": "HhJ2GYybmx3l"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the sum of importance scores\n",
        "f_list = sorted(zip(map(lambda x: round(x, 4), importances), features), reverse=True)\n",
        "Sum = 0\n",
        "fs = []\n",
        "for i in range(0, len(f_list)):\n",
        "    Sum = Sum + f_list[i][0]\n",
        "    fs.append(f_list[i][1])"
      ],
      "metadata": {
        "id": "Z8ceC_GQnHPM"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the important features from top to bottom until the accumulated importance reaches 90%\n",
        "f_list2 = sorted(zip(map(lambda x: round(x, 4), importances/Sum), features), reverse=True)\n",
        "Sum2 = 0\n",
        "fs = []\n",
        "for i in range(0, len(f_list2)):\n",
        "    Sum2 = Sum2 + f_list2[i][0]\n",
        "    fs.append(f_list2[i][1])\n",
        "    if Sum2>=0.9:\n",
        "        break"
      ],
      "metadata": {
        "id": "sFUJJmpInIP0"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fs = df[fs].values"
      ],
      "metadata": {
        "id": "0ktGMg_enJlU"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fs.shape"
      ],
      "metadata": {
        "id": "rA0X5Tv6nKq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54bdadaa-b157-4f39-ff5f-23f4deaba2c7"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25490, 45)"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature selection by Fast Correlation Based Filter (FCBF)\n"
      ],
      "metadata": {
        "id": "_Nq-i1gSnNoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimizer"
      ],
      "metadata": {
        "id": "RUSgzfmKnNH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e0687e-6896-404f-81a4-706f12f936c2"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-optimizer in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.5.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (25.7.0)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimizer) (1.6.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimizer) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->scikit-optimizer) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import gp_minimize\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "def fcbf_objective(threshold, X, y, clf=RandomForestClassifier(random_state=42)):\n",
        "    selector = FCBF(th=threshold)\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "    if X_selected.shape[1] == 0:\n",
        "        return 1.0\n",
        "    score = cross_val_score(clf, X_selected, y, cv=3, scoring=\"accuracy\").mean()\n",
        "    return -score"
      ],
      "metadata": {
        "id": "P47o6mWfnQe1"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from FCBF_module import FCBF, FCBFK, FCBFiP, get_i\n",
        "\n",
        "res = gp_minimize(\n",
        "    lambda th: fcbf_objective(th[0], X_fs, y),\n",
        "    dimensions=[(0.01, 0.5)],\n",
        "    n_calls=20,\n",
        "    random_state=42,\n",
        "    acq_func='EI',\n",
        ")\n",
        "\n",
        "best_threshold = res.x[0]\n",
        "print(\"Best threshold:\", best_threshold)\n",
        "print(\"Best accuracy:\", -res.fun)"
      ],
      "metadata": {
        "id": "WoTwizkdnRoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "494c264e-9ecb-467e-a900-4eff21b71be5"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.09988304703442026\n",
            "Best accuracy: 0.8891325820397746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from FCBF_module import FCBF, FCBFK, FCBFiP, get_i\n",
        "fcbf = FCBF(th = best_threshold)"
      ],
      "metadata": {
        "id": "k7f6adG3nS3k"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_fss = fcbf.fit_transform(X_fs,y)"
      ],
      "metadata": {
        "id": "pq-8md4PnT_E"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  kernel principal component analysis (KPCA)"
      ],
      "metadata": {
        "id": "TVZK6sdvnXlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import gp_minimize\n",
        "from skopt.space import Integer, Categorical\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def kpca_objective(params, X, y):\n",
        "    n_components, kernel = params\n",
        "    n_components = min(n_components, X.shape[1] - 1)\n",
        "    if n_components < 1: n_components = 1\n",
        "    try:\n",
        "        kpca = KernelPCA(n_components=n_components, kernel=kernel, fit_inverse_transform=False, random_state=42)\n",
        "        X_kpca = kpca.fit_transform(X)\n",
        "        clf = RandomForestClassifier(random_state=42)\n",
        "        score = cross_val_score(clf, X_kpca, y, cv=3, scoring=\"accuracy\").mean()\n",
        "        return -score\n",
        "    except Exception as e:\n",
        "        return 1.0\n",
        "\n",
        "search_space = [\n",
        "    Integer(2, 50),\n",
        "    Categorical(['rbf', 'poly'])\n",
        "]\n",
        "\n",
        "result = gp_minimize(\n",
        "    lambda params: kpca_objective(params, X_fss, y),\n",
        "    search_space,\n",
        "    n_calls=20,\n",
        "    random_state=42,\n",
        "    acq_func='EI'\n",
        ")\n",
        "\n",
        "best_n_components, best_kernel = result.x\n",
        "print(f\"Best n_components: {best_n_components}, Best kernel: {best_kernel}\")\n",
        "print(f\"Best accuracy: {-result.fun:.4f}\")"
      ],
      "metadata": {
        "id": "6T05jbegnVEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d9a38c-bce6-4f55-f620-a33d178f2208"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best n_components: 40, Best kernel: poly\n",
            "Best accuracy: 0.8892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kpca = KernelPCA(n_components=best_n_components, kernel=best_kernel, random_state=42)\n",
        "X_kpca = kpca.fit_transform(X_fss)"
      ],
      "metadata": {
        "id": "-27MFOlbnZlm"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-test split after feature selection"
      ],
      "metadata": {
        "id": "I-YSMec6ncrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_idx = df.query('Label == 0').index\n",
        "df_val_test_idx = df.drop(df_train_idx).index"
      ],
      "metadata": {
        "id": "dGT0kP25Gy34"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_kpca[df_train_idx]\n",
        "y_train = np.zeros(len(X_train), dtype=int)"
      ],
      "metadata": {
        "id": "yBnFDUW2G0WY"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "1_WKlr5HG1UR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3bed46d-586c-4f3e-ffbf-17e3e5b09c8e"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10226, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "iI--UTy9G3yL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cca6c1b-8963-431c-fdf8-ab6a5629a70b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10226,)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_test_data = X_kpca[df_val_test_idx]\n",
        "val_test_labels = df.loc[df_val_test_idx, 'Label']"
      ],
      "metadata": {
        "id": "B9OF_bT3G6pZ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_val_test_idx)"
      ],
      "metadata": {
        "id": "NY-DxhzlO_sN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79237fcf-a752-4fd5-ab29-f18819a0082c"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15264"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_test_data.shape"
      ],
      "metadata": {
        "id": "w10GYnFZPnJg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d986439-f717-4762-92d7-ef135050064b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15264, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_test_labels.value_counts()"
      ],
      "metadata": {
        "id": "4P4nJYkUkJHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "871a3706-c0e9-44b9-d29b-e679eccca1b5"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Label\n",
              "1    15146\n",
              "6      118\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if isinstance(X_val_test_data, (pd.Series, pd.DataFrame)):\n",
        "    X_val_test_data = X_val_test_data.to_numpy()\n",
        "\n",
        "if isinstance(val_test_labels, (pd.Series, pd.DataFrame)):\n",
        "    val_test_labels = val_test_labels.to_numpy()\n",
        "\n",
        "indices_1 = np.where(val_test_labels == 1)[0]\n",
        "indices_13 = np.where(val_test_labels == 6)[0]\n",
        "\n",
        "X_test_label_13 = X_val_test_data[indices_13]\n",
        "y_test_label_13 = val_test_labels[indices_13]\n",
        "\n",
        "num_label_13 = len(y_test_label_13)\n",
        "\n",
        "indices_0_train = np.where(y_train == 0)[0]\n",
        "num_label_0_train = len(indices_0_train)\n",
        "\n",
        "np.random.seed(42)\n",
        "if num_label_0_train >= num_label_13:\n",
        "    indices_0_test_sample = np.random.choice(indices_0_train, size=num_label_13, replace=False)\n",
        "else:\n",
        "    indices_0_test_sample = np.random.choice(indices_0_train, size=num_label_0_train, replace=False)\n",
        "\n",
        "X_test_label_0 = X_train[indices_0_test_sample]\n",
        "y_test_label_0 = y_train[indices_0_test_sample]\n",
        "\n",
        "X_test = np.concatenate([X_test_label_13, X_test_label_0], axis=0)\n",
        "y_test = np.concatenate([y_test_label_13, y_test_label_0], axis=0)\n",
        "\n",
        "X_val_label_1 = X_val_test_data[indices_1]\n",
        "y_val_label_1 = val_test_labels[indices_1]\n",
        "\n",
        "remaining_indices_0_train = np.setdiff1d(indices_0_train, indices_0_test_sample)\n",
        "\n",
        "X_val_label_0 = X_train[remaining_indices_0_train]\n",
        "y_val_label_0 = y_train[remaining_indices_0_train]\n",
        "\n",
        "num_val_label_0 = len(y_val_label_0)\n",
        "num_val_label_1 = len(y_val_label_1)\n",
        "\n",
        "np.random.seed(42)\n",
        "if num_val_label_1 >= num_val_label_0:\n",
        "    indices_1_val_sample = np.random.choice(num_val_label_1, size=num_val_label_0, replace=False)\n",
        "    X_val_label_1_final = X_val_label_1[indices_1_val_sample]\n",
        "    y_val_label_1_final = y_val_label_1[indices_1_val_sample]\n",
        "else:\n",
        "    X_val_label_1_final = X_val_label_1\n",
        "    y_val_label_1_final = y_val_label_1\n",
        "\n",
        "X_val = np.concatenate([X_val_label_0, X_val_label_1_final], axis=0)\n",
        "y_val = np.concatenate([y_val_label_0, y_val_label_1_final], axis=0)\n",
        "\n",
        "print(\"Shape of final X_val:\", X_val.shape)\n",
        "print(\"Shape of final y_val:\", y_val.shape)\n",
        "unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
        "print(\"y_val label counts:\", dict(zip(unique_val, counts_val)))\n",
        "\n",
        "print(\"\\nShape of final X_test:\", X_test.shape)\n",
        "print(\"Shape of final y_test:\", y_test.shape)\n",
        "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "print(\"y_test label counts:\", dict(zip(unique_test, counts_test)))"
      ],
      "metadata": {
        "id": "EkI3DgTzRw_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d9b710-7e92-4b3a-b07d-7924b9e0650e"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of final X_val: (20216, 40)\n",
            "Shape of final y_val: (20216,)\n",
            "y_val label counts: {np.int64(0): np.int64(10108), np.int64(1): np.int64(10108)}\n",
            "\n",
            "Shape of final X_test: (236, 40)\n",
            "Shape of final y_test: (236,)\n",
            "y_test label counts: {np.int64(0): np.int64(118), np.int64(6): np.int64(118)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_val[y_val != 0] = 1\n",
        "\n",
        "y_test[y_test != 0] = 1\n"
      ],
      "metadata": {
        "id": "-8O3c8sNmuCF"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R35HcAwJnHO",
        "outputId": "c814667b-ecd5-41cc-a0de-95339e6a945b"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20216, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "std_scaler = MinMaxScaler()\n",
        "std_scaler = std_scaler.fit(X_train)\n",
        "\n",
        "X_train = std_scaler.transform(X_train)\n",
        "X_val = std_scaler.transform(X_val)\n",
        "X_test = std_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "QvUcy_l_cLGa"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply the Autoencoder model with biased classifiers"
      ],
      "metadata": {
        "id": "fEg4XjxKobsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementação do Early Stopping\n",
        "class EarlyStopping:\n",
        "  def __init__(self, patience=7, delta=0, verbose=True, path='checkpoint.pt'):\n",
        "      self.patience = patience\n",
        "      self.delta = delta\n",
        "      self.verbose = verbose\n",
        "      self.counter = 0\n",
        "      self.early_stop = False\n",
        "      self.val_min_loss = np.inf\n",
        "      self.path = path\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "    if val_loss < self.val_min_loss - self.delta:   # Caso a loss da validação reduza, vamos salvar o modelo e nova loss mínima\n",
        "      self.save_checkpoint(val_loss, model)\n",
        "      self.counter = 0\n",
        "    else:                                           # Caso a loss da validação NÃO reduza, vamos incrementar o contador da paciencia\n",
        "      self.counter += 1\n",
        "      print(f'EarlyStopping counter: {self.counter} out of {self.patience}. Current validation loss: {val_loss:.5f}')\n",
        "      if self.counter >= self.patience:\n",
        "          self.early_stop = True\n",
        "\n",
        "  def save_checkpoint(self, val_loss, model):\n",
        "    if self.verbose:\n",
        "        print(f'Validation loss decreased ({self.val_min_loss:.5f} --> {val_loss:.5f}).  Saving model ...')\n",
        "    torch.save(model.state_dict(), self.path)\n",
        "    self.val_min_loss = val_loss"
      ],
      "metadata": {
        "id": "0nAxBs0PoZul"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, in_features, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.early_stopping = None\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_features, 24),\n",
        "            nn.BatchNorm1d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(24, 8),\n",
        "            nn.BatchNorm1d(8),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(8, 24),\n",
        "            nn.BatchNorm1d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(24, in_features),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        encoded = self.encoder(X)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        "\n",
        "    def compile(self, learning_rate):\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def fit_transform(self, X_train, num_epochs, batch_size,\n",
        "                      X_val=None, patience=None, delta=None, device=None):\n",
        "        \"\"\"\n",
        "        Trains the autoencoder and returns the encoded representation of X_train.\n",
        "        \"\"\"\n",
        "        if X_val is not None and patience is not None and delta is not None:\n",
        "            print(f'Using early stopping with patience={patience} and delta={delta}')\n",
        "            self.early_stopping = EarlyStopping(patience, delta)\n",
        "\n",
        "        val_avg_losses = []\n",
        "        train_avg_losses = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_losses = []\n",
        "            self.train()\n",
        "            for batch in range(0, len(X_train), batch_size):\n",
        "                batch_X = X_train[batch:(batch + batch_size)]\n",
        "                encoded, decoded = self.forward(batch_X)\n",
        "\n",
        "                loss = self.criterion(decoded, batch_X)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "            train_avg_loss = np.mean(train_losses)\n",
        "            train_avg_losses.append(train_avg_loss)\n",
        "            print(f'Epoch#{epoch+1}: Train Average Loss = {train_avg_loss:.5f}')\n",
        "\n",
        "            if self.early_stopping is not None:\n",
        "                val_losses = []\n",
        "                self.eval()\n",
        "                with torch.no_grad():\n",
        "                    for batch in range(0, len(X_val), batch_size):\n",
        "                        batch_X = X_val[batch:(batch + batch_size)]\n",
        "                        _, decoded = self.forward(batch_X)\n",
        "                        val_loss = self.criterion(decoded, batch_X)\n",
        "                        val_losses.append(val_loss.item())\n",
        "                val_avg_loss = np.mean(val_losses)\n",
        "                val_avg_losses.append(val_avg_loss)\n",
        "                self.early_stopping(val_avg_loss, self)\n",
        "                if self.early_stopping.early_stop:\n",
        "                    print(f'Stopped by early stopping at epoch {epoch+1}')\n",
        "                    break\n",
        "\n",
        "        if self.early_stopping is not None:\n",
        "            self.load_state_dict(torch.load('checkpoint.pt', map_location=device))\n",
        "\n",
        "        self.eval()\n",
        "        return self.transform(X_train, batch_size)\n",
        "\n",
        "    def transform(self, X, batch_size=256):\n",
        "        \"\"\"\n",
        "        Returns the encoded representation of the input X.\n",
        "        \"\"\"\n",
        "        encoded_list = []\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in range(0, len(X), batch_size):\n",
        "                batch_X = X[batch:(batch + batch_size)]\n",
        "                encoded, _ = self.forward(batch_X)\n",
        "                encoded_list.append(encoded.cpu())\n",
        "        return torch.cat(encoded_list, dim=0)\n"
      ],
      "metadata": {
        "id": "DPejKYPIofpF"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer, Real\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class SklearnAutoencoder(BaseEstimator):\n",
        "    def __init__(self, in_features, hidden1=24, hidden2=8, dropout=0.2, lr=1e-3, num_epochs=50, batch_size=256):\n",
        "        self.in_features = in_features\n",
        "        self.hidden1 = hidden1\n",
        "        self.hidden2 = hidden2\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        self.model = Autoencoder(self.in_features, dropout_rate=self.dropout)\n",
        "        self.model.encoder = nn.Sequential(\n",
        "            nn.Linear(self.in_features, self.hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden1, self.hidden2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.model.decoder = nn.Sequential(\n",
        "            nn.Linear(self.hidden2, self.hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(self.dropout),\n",
        "            nn.Linear(self.hidden1, self.in_features),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.model.to(self.device)\n",
        "        self.model.compile(self.lr)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        _ = self.model.fit_transform(X_tensor, self.num_epochs, self.batch_size)\n",
        "        return self\n",
        "\n",
        "    def score(self, X, y=None):\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        _, decoded = self.model(X_tensor)\n",
        "        decoded = decoded.detach().cpu().numpy()\n",
        "        loss = np.mean((X - decoded) ** 2)\n",
        "        return -loss\n"
      ],
      "metadata": {
        "id": "Zh8TShj7oh9c"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "def ae_scorer(estimator, X, y=None):\n",
        "    return estimator.score(X)\n",
        "\n",
        "search_space = {\n",
        "    \"hidden1\": Integer(32, 256),   # bigger first hidden layer\n",
        "    \"hidden2\": Integer(8, 128),    # bigger bottleneck\n",
        "    \"dropout\": Real(0.0, 0.5),\n",
        "    \"lr\": Real(1e-4, 5e-3, \"log-uniform\")\n",
        "}\n",
        "\n",
        "opt_ae = BayesSearchCV(\n",
        "    SklearnAutoencoder(in_features=X_train.shape[1]),\n",
        "    search_space,\n",
        "    n_iter=50,\n",
        "    cv=10,\n",
        "    n_jobs=-1,\n",
        "    scoring=ae_scorer,   # use the custom scorer\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "opt_ae.fit(X_train)\n",
        "best_ae = opt_ae.best_estimator_.model\n"
      ],
      "metadata": {
        "id": "Yt8egvrvokFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0df9c19-a322-4867-dfd4-eb65103d114b"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch#1: Train Average Loss = 0.22459\n",
            "Epoch#2: Train Average Loss = 0.15892\n",
            "Epoch#3: Train Average Loss = 0.04507\n",
            "Epoch#4: Train Average Loss = 0.00648\n",
            "Epoch#5: Train Average Loss = 0.00347\n",
            "Epoch#6: Train Average Loss = 0.00289\n",
            "Epoch#7: Train Average Loss = 0.00266\n",
            "Epoch#8: Train Average Loss = 0.00250\n",
            "Epoch#9: Train Average Loss = 0.00241\n",
            "Epoch#10: Train Average Loss = 0.00236\n",
            "Epoch#11: Train Average Loss = 0.00230\n",
            "Epoch#12: Train Average Loss = 0.00227\n",
            "Epoch#13: Train Average Loss = 0.00225\n",
            "Epoch#14: Train Average Loss = 0.00222\n",
            "Epoch#15: Train Average Loss = 0.00220\n",
            "Epoch#16: Train Average Loss = 0.00219\n",
            "Epoch#17: Train Average Loss = 0.00217\n",
            "Epoch#18: Train Average Loss = 0.00216\n",
            "Epoch#19: Train Average Loss = 0.00214\n",
            "Epoch#20: Train Average Loss = 0.00214\n",
            "Epoch#21: Train Average Loss = 0.00213\n",
            "Epoch#22: Train Average Loss = 0.00212\n",
            "Epoch#23: Train Average Loss = 0.00210\n",
            "Epoch#24: Train Average Loss = 0.00209\n",
            "Epoch#25: Train Average Loss = 0.00207\n",
            "Epoch#26: Train Average Loss = 0.00207\n",
            "Epoch#27: Train Average Loss = 0.00205\n",
            "Epoch#28: Train Average Loss = 0.00202\n",
            "Epoch#29: Train Average Loss = 0.00202\n",
            "Epoch#30: Train Average Loss = 0.00200\n",
            "Epoch#31: Train Average Loss = 0.00199\n",
            "Epoch#32: Train Average Loss = 0.00196\n",
            "Epoch#33: Train Average Loss = 0.00196\n",
            "Epoch#34: Train Average Loss = 0.00193\n",
            "Epoch#35: Train Average Loss = 0.00190\n",
            "Epoch#36: Train Average Loss = 0.00188\n",
            "Epoch#37: Train Average Loss = 0.00185\n",
            "Epoch#38: Train Average Loss = 0.00183\n",
            "Epoch#39: Train Average Loss = 0.00179\n",
            "Epoch#40: Train Average Loss = 0.00176\n",
            "Epoch#41: Train Average Loss = 0.00172\n",
            "Epoch#42: Train Average Loss = 0.00169\n",
            "Epoch#43: Train Average Loss = 0.00166\n",
            "Epoch#44: Train Average Loss = 0.00162\n",
            "Epoch#45: Train Average Loss = 0.00157\n",
            "Epoch#46: Train Average Loss = 0.00154\n",
            "Epoch#47: Train Average Loss = 0.00149\n",
            "Epoch#48: Train Average Loss = 0.00146\n",
            "Epoch#49: Train Average Loss = 0.00142\n",
            "Epoch#50: Train Average Loss = 0.00138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Y4uFg3M1nGe0"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_overall_metrics(y_true, y_pred):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "  acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "  tpr = tp/(tp+fn)\n",
        "  fpr = fp/(fp+tn)\n",
        "  precision = tp/(tp+fp)\n",
        "  f1 = (2*tpr*precision)/(tpr+precision)\n",
        "  return {'acc':acc,'tpr':tpr,'fpr':fpr,'precision':precision,'f1-score':f1}"
      ],
      "metadata": {
        "id": "ifNLmQFrne_0"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    recall_score\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "def Anomaly_IDS(best_ae, input_dim, X_train, X_test, y_train, y_test, X_val, y_val, n, uncertainty_margin=0.01,b=100, num_epochs=50, batch_size=256):\n",
        "\n",
        "    ae = best_ae\n",
        "    X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
        "    _ = ae.fit_transform(X_train_torch, num_epochs, batch_size)\n",
        "\n",
        "    def get_autoencoder_anomaly_scores(model, X):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_torch = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "            # The model returns (latent_representation, reconstruction)\n",
        "            _, reconstructed_X = model(X_torch)\n",
        "            # Calculate Mean Squared Error for each sample\n",
        "            anomaly_scores = torch.mean(torch.pow(X_torch - reconstructed_X, 2), axis=1)\n",
        "            return anomaly_scores.detach().cpu().numpy()\n",
        "\n",
        "    val_anomaly_scores = get_autoencoder_anomaly_scores(ae, X_val)\n",
        "    fpr, tpr, thresholds = roc_curve(y_val, val_anomaly_scores)\n",
        "\n",
        "    print('Fpr', fpr)\n",
        "    print('Tpr', tpr)\n",
        "    print('Thresholds', thresholds)\n",
        "\n",
        "    df_roc = pd.DataFrame({\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'thresholds': thresholds\n",
        "    })\n",
        "\n",
        "    df_roc['youden_index'] = df_roc['tpr'] - df_roc['fpr']\n",
        "    best_row = df_roc.loc[df_roc['youden_index'].idxmax()]\n",
        "    optimal_threshold = best_row['thresholds']\n",
        "\n",
        "    test_anomaly_scores = get_autoencoder_anomaly_scores(ae, X_test)\n",
        "\n",
        "    y_pred_ae = (test_anomaly_scores >= optimal_threshold).astype(int)\n",
        "\n",
        "    print(\"Autoencoder Performance (threshold-based):\")\n",
        "    print(classification_report(y_test, y_pred_ae))\n",
        "    cm_ae = confusion_matrix(y_test, y_pred_ae)\n",
        "    tn, fp, fn, tp = cm_ae.ravel()\n",
        "    dr_ae = tp/(tp+fn) if tp+fn>0 else 0\n",
        "    far_ae = fp/(fp+tn) if fp+tn>0 else 0\n",
        "    acc_ae = accuracy_score(y_test, y_pred_ae)\n",
        "    print(f\"  Acc: {acc_ae:.4f}, DR: {dr_ae:.4f}, FAR: {far_ae:.4f}\\n  CM:\\n{cm_ae}\")\n",
        "\n",
        "    val_anomaly_scores = get_autoencoder_anomaly_scores(ae, X_val)\n",
        "\n",
        "    # 1. Identify False Positives (FP) and False Negatives (FN) from the training set\n",
        "    fp_idx = np.where((val_anomaly_scores >= optimal_threshold) & (y_val == 0))[0]\n",
        "    fn_idx = np.where((val_anomaly_scores < optimal_threshold) & (y_val == 1))[0]\n",
        "\n",
        "    X_fp_train = X_val[fp_idx] # Normal samples misclassified as Attack\n",
        "    X_fn_train = X_val[fn_idx] # Attack samples misclassified as Normal\n",
        "\n",
        "    rfp = None\n",
        "    rfn = None\n",
        "    cv_splits = 5\n",
        "\n",
        "    if len(X_fp_train) > 0:\n",
        "        X_attacks_for_fp = X_train[y_train == 1]\n",
        "        if len(X_attacks_for_fp) > 0:\n",
        "            attack_samples_for_fp = X_attacks_for_fp[np.random.choice(len(X_attacks_for_fp), size=len(X_fp_train), replace=True)]\n",
        "            Xp = np.concatenate([X_fp_train, attack_samples_for_fp])\n",
        "            yp = np.concatenate([np.zeros(len(X_fp_train)), np.ones(len(attack_samples_for_fp))])\n",
        "            if min(np.bincount(yp.astype(int))) >= cv_splits:\n",
        "                # Use BayesSearchCV if enough samples exist\n",
        "                opt_rfp = BayesSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': Integer(10, 200), 'max_depth': Integer(3, 50), 'min_samples_split': Integer(2, 10)}, n_iter=20, cv=StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42), scoring='f1', n_jobs=-1, random_state=42)\n",
        "                opt_rfp.fit(Xp, yp)\n",
        "                rfp = opt_rfp.best_estimator_\n",
        "            else:\n",
        "                rfp = RandomForestClassifier(random_state=42).fit(Xp, yp)\n",
        "\n",
        "    if len(X_fn_train) > 0:\n",
        "        X_normals_for_fn = X_train[y_train == 0]\n",
        "        if len(X_normals_for_fn) > 0:\n",
        "            normal_samples_for_fn = X_normals_for_fn[np.random.choice(len(X_normals_for_fn), size=len(X_fn_train), replace=True)]\n",
        "            Xn = np.concatenate([X_fn_train, normal_samples_for_fn])\n",
        "            yn = np.concatenate([np.ones(len(X_fn_train)), np.zeros(len(normal_samples_for_fn))])\n",
        "            if min(np.bincount(yn.astype(int))) >= cv_splits:\n",
        "                # Use BayesSearchCV if enough samples exist\n",
        "                opt_rfn = BayesSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': Integer(10, 200), 'max_depth': Integer(3, 50), 'min_samples_split': Integer(2, 10)}, n_iter=20, cv=StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42), scoring='f1', n_jobs=-1, random_state=42)\n",
        "                opt_rfn.fit(Xn, yn)\n",
        "                rfn = opt_rfn.best_estimator_\n",
        "            else:\n",
        "                rfn = RandomForestClassifier(random_state=42).fit(Xn, yn)\n",
        "\n",
        "\n",
        "    y_final = y_pred_ae.copy()\n",
        "    print(\"\\n--- Applying Targeted Correction ---\")\n",
        "\n",
        "    if rfn is not None:\n",
        "        rfn_check_indices = np.where(\n",
        "            (y_pred_ae == 0) &\n",
        "            (test_anomaly_scores > optimal_threshold - uncertainty_margin)\n",
        "        )[0]\n",
        "        print(f\"Found {len(rfn_check_indices)} uncertain 'Normal' samples to re-classify with RFN.\")\n",
        "        if len(rfn_check_indices) > 0:\n",
        "            rfn_predictions = rfn.predict(X_test[rfn_check_indices])\n",
        "            y_final[rfn_check_indices] = rfn_predictions\n",
        "\n",
        "    if rfp is not None:\n",
        "        rfp_check_indices = np.where(\n",
        "            (y_pred_ae == 1) &\n",
        "            (test_anomaly_scores < optimal_threshold + uncertainty_margin)\n",
        "        )[0]\n",
        "        print(f\"Found {len(rfp_check_indices)} uncertain 'Attack' samples to re-classify with RFP.\")\n",
        "        if len(rfp_check_indices) > 0:\n",
        "            rfp_predictions = rfp.predict(X_test[rfp_check_indices])\n",
        "            y_final[rfp_check_indices] = rfp_predictions\n",
        "\n",
        "    print(classification_report(y_test, y_final, target_names=['Normal', 'Attack']))\n",
        "    cm_final = confusion_matrix(y_test, y_final)\n",
        "\n",
        "    tn_f, fp_f, fn_f, tp_f = cm_final.ravel()\n",
        "    acc_final = accuracy_score(y_test, y_final)\n",
        "    dr_final = tp_f / (tp_f + fn_f) if (tp_f + fn_f) > 0 else 0\n",
        "    far_final = fp_f / (fp_f + tn_f) if (fp_f + tn_f) > 0 else 0\n",
        "\n",
        "    print(f\"  Acc: {acc_final:.4f}, DR: {dr_final:.4f}, FAR: {far_final:.4f}\\n  CM:\\n{cm_final}\")\n",
        "\n",
        "    return acc_final, dr_final, far_final, cm_final\n",
        "\n"
      ],
      "metadata": {
        "id": "0ffL9boeohDl"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(y_val).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "QYZ0nb-2M8_a",
        "outputId": "f0ed7b75-c1bf-4f4e-d14f-14c77a526cb5"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    10108\n",
              "1    10108\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10108</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score, recall_score, roc_curve\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def Anomaly_IDS(best_ae, X_train, y_train, X_test, y_test, X_val, y_val, num_epochs=50, batch_size=256):\n",
        "    X_train_benign = X_train[y_train == 0]\n",
        "    X_train_benign_torch = torch.tensor(X_train_benign, dtype=torch.float32)\n",
        "    ae = best_ae\n",
        "    _ = ae.fit_transform(X_train_benign_torch, num_epochs, batch_size)\n",
        "\n",
        "    def get_autoencoder_anomaly_scores(model, X):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            X_torch = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "            _, reconstructed_X = model(X_torch)\n",
        "            anomaly_scores = torch.mean(torch.pow(X_torch - reconstructed_X, 2), axis=1)\n",
        "            return anomaly_scores.detach().cpu().numpy()\n",
        "\n",
        "    X_val_train, X_val_tune, y_val_train, y_val_tune = train_test_split(\n",
        "        X_val, y_val, test_size=0.4, random_state=42, stratify=y_val\n",
        "    )\n",
        "\n",
        "    val_train_scores = get_autoencoder_anomaly_scores(ae, X_val_train)\n",
        "    fpr, tpr, thresholds = roc_curve(y_val_train, val_train_scores)\n",
        "\n",
        "    df_roc = pd.DataFrame({'tpr': tpr, 'fpr': fpr, 'thresholds': thresholds})\n",
        "    df_roc['youden_index'] = df_roc['tpr'] - df_roc['fpr']\n",
        "    # Handle cases where max index might not be unique or thresholds are infinite\n",
        "    df_roc.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_roc.dropna(inplace=True)\n",
        "    optimal_threshold = df_roc.loc[df_roc['youden_index'].idxmax()]['thresholds']\n",
        "\n",
        "    fp_idx = np.where((val_train_scores >= optimal_threshold) & (y_val_train == 0))[0]\n",
        "    fn_idx = np.where((val_train_scores < optimal_threshold) & (y_val_train == 1))[0]\n",
        "\n",
        "    X_fp_train = X_val_train[fp_idx]\n",
        "    X_fn_train = X_val_train[fn_idx]\n",
        "\n",
        "    # (The logic for training rfp and rfn remains identical, just using the new data)\n",
        "    rfp, rfn = train_specialist_classifiers(X_fp_train, X_fn_train, X_train, y_train)\n",
        "\n",
        "    # --- Step 2: Tune the uncertainty_margin on the val_tune set ---\n",
        "    print(\"\\nTuning the uncertainty margin...\")\n",
        "    # Define candidate margins as a percentage of the optimal threshold\n",
        "    margin_candidates_relative = np.linspace(0.01, 0.50, 10) # e.g., 1% to 50%\n",
        "    margin_candidates = [relative * optimal_threshold for relative in margin_candidates_relative]\n",
        "\n",
        "    best_margin = 0\n",
        "    best_f1 = -1\n",
        "\n",
        "    val_tune_scores = get_autoencoder_anomaly_scores(ae, X_val_tune)\n",
        "    y_pred_ae_tune = (val_tune_scores >= optimal_threshold).astype(int)\n",
        "\n",
        "    for margin in margin_candidates:\n",
        "        y_final_tune = apply_correction(\n",
        "            y_pred_ae_tune, val_tune_scores, X_val_tune, rfp, rfn, optimal_threshold, margin\n",
        "        )\n",
        "        current_f1 = f1_score(y_val_tune, y_final_tune)\n",
        "\n",
        "        if current_f1 > best_f1:\n",
        "            best_f1 = current_f1\n",
        "            best_margin = margin\n",
        "\n",
        "    print(f\"Optimal uncertainty margin found: {best_margin:.6f} (Resulting F1 on tune set: {best_f1:.4f})\")\n",
        "\n",
        "    # --- Step 3: Final evaluation on the unseen test set using the best margin ---\n",
        "    print(\"\\n--- Final Evaluation on Test Set ---\")\n",
        "    test_anomaly_scores = get_autoencoder_anomaly_scores(ae, X_test)\n",
        "    y_pred_ae_test = (test_anomaly_scores >= optimal_threshold).astype(int)\n",
        "\n",
        "    print(\"\\nAutoencoder Performance (baseline):\")\n",
        "    print_metrics(y_test, y_pred_ae_test)\n",
        "\n",
        "    print(\"\\n--- Applying Targeted Correction with Tuned Margin ---\")\n",
        "    y_final_test = apply_correction(\n",
        "        y_pred_ae_test, test_anomaly_scores, X_test, rfp, rfn, optimal_threshold, best_margin\n",
        "    )\n",
        "\n",
        "    print(\"\\nFinal Hybrid Model Performance:\")\n",
        "    tn_f, fp_f, fn_f, tp_f, acc_final, dr_final, far_final, cm_final = print_metrics(y_test, y_final_test, return_raw=True)\n",
        "\n",
        "    return acc_final, dr_final, far_final, cm_final\n",
        "\n",
        "# --- Helper function to avoid code repetition ---\n",
        "def train_specialist_classifiers(X_fp_train, X_fn_train, X_train, y_train):\n",
        "    rfp = None\n",
        "    rfn = None\n",
        "    cv_splits = 5\n",
        "\n",
        "    # RFP Training Logic (moved here from the main function)\n",
        "    if len(X_fp_train) > 0:\n",
        "        X_attacks_for_fp = X_train[y_train == 1]\n",
        "        if len(X_attacks_for_fp) > 0:\n",
        "            attack_samples_for_fp = X_attacks_for_fp[np.random.choice(len(X_attacks_for_fp), size=len(X_fp_train), replace=True)]\n",
        "            Xp = np.concatenate([X_fp_train, attack_samples_for_fp])\n",
        "            yp = np.concatenate([np.zeros(len(X_fp_train)), np.ones(len(attack_samples_for_fp))])\n",
        "            if min(np.bincount(yp.astype(int))) >= cv_splits:\n",
        "                opt_rfp = BayesSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': Integer(10, 200), 'max_depth': Integer(3, 50), 'min_samples_split': Integer(2, 10)}, n_iter=20, cv=StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42), scoring='f1', n_jobs=-1, random_state=42)\n",
        "                opt_rfp.fit(Xp, yp)\n",
        "                rfp = opt_rfp.best_estimator_\n",
        "            else:\n",
        "                rfp = RandomForestClassifier(random_state=42).fit(Xp, yp)\n",
        "\n",
        "    # RFN Training Logic (moved here from the main function)\n",
        "    if len(X_fn_train) > 0:\n",
        "        X_normals_for_fn = X_train[y_train == 0]\n",
        "        if len(X_normals_for_fn) > 0:\n",
        "            normal_samples_for_fn = X_normals_for_fn[np.random.choice(len(X_normals_for_fn), size=len(X_fn_train), replace=True)]\n",
        "            Xn = np.concatenate([X_fn_train, normal_samples_for_fn])\n",
        "            yn = np.concatenate([np.ones(len(X_fn_train)), np.zeros(len(normal_samples_for_fn))])\n",
        "            if min(np.bincount(yn.astype(int))) >= cv_splits:\n",
        "                opt_rfn = BayesSearchCV(RandomForestClassifier(random_state=42), {'n_estimators': Integer(10, 200), 'max_depth': Integer(3, 50), 'min_samples_split': Integer(2, 10)}, n_iter=20, cv=StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42), scoring='f1', n_jobs=-1, random_state=42)\n",
        "                opt_rfn.fit(Xn, yn)\n",
        "                rfn = opt_rfn.best_estimator_\n",
        "            else:\n",
        "                rfn = RandomForestClassifier(random_state=42).fit(Xn, yn)\n",
        "\n",
        "    return rfp, rfn\n",
        "\n",
        "def apply_correction(y_pred_initial, anomaly_scores, X_data, rfp, rfn, threshold, margin):\n",
        "    y_final = y_pred_initial.copy()\n",
        "\n",
        "    # Apply RFN to uncertain 'Normal' samples\n",
        "    if rfn is not None:\n",
        "        rfn_indices = np.where((y_pred_initial == 0) & (anomaly_scores > threshold - margin))[0]\n",
        "        if len(rfn_indices) > 0:\n",
        "            rfn_preds = rfn.predict(X_data[rfn_indices])\n",
        "            y_final[rfn_indices] = rfn_preds\n",
        "\n",
        "    # Apply RFP to uncertain 'Attack' samples\n",
        "    if rfp is not None:\n",
        "        rfp_indices = np.where((y_pred_initial == 1) & (anomaly_scores < threshold + margin))[0]\n",
        "        if len(rfp_indices) > 0:\n",
        "            rfp_preds = rfp.predict(X_data[rfp_indices])\n",
        "            y_final[rfp_indices] = rfp_preds\n",
        "\n",
        "    return y_final\n",
        "\n",
        "def print_metrics(y_true, y_pred, return_raw=False):\n",
        "    print(classification_report(y_true, y_pred, target_names=['Normal', 'Attack']))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    dr = recall_score(y_true, y_pred) # Detection Rate is just recall\n",
        "    far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    print(f\"  Acc: {acc:.4f}, DR: {dr:.4f}, FAR: {far:.4f}\\n  CM:\\n{cm}\")\n",
        "    if return_raw:\n",
        "        return tn, fp, fn, tp, acc, dr, far, cm"
      ],
      "metadata": {
        "id": "QDcqQAGn17yM"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Anomaly_IDS(\n",
        "    best_ae=best_ae,\n",
        "    X_train=X_train,   # convert to numpy\n",
        "    X_test=X_test,\n",
        "    y_train=y_train,   # convert to numpy\n",
        "    y_test=y_test,\n",
        "    X_val=X_val,\n",
        "    y_val=y_val\n",
        ")"
      ],
      "metadata": {
        "id": "X64donH9olfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65007d3b-aacc-4876-dc9a-357a46da128d"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch#1: Train Average Loss = 0.00136\n",
            "Epoch#2: Train Average Loss = 0.00133\n",
            "Epoch#3: Train Average Loss = 0.00131\n",
            "Epoch#4: Train Average Loss = 0.00128\n",
            "Epoch#5: Train Average Loss = 0.00126\n",
            "Epoch#6: Train Average Loss = 0.00123\n",
            "Epoch#7: Train Average Loss = 0.00120\n",
            "Epoch#8: Train Average Loss = 0.00118\n",
            "Epoch#9: Train Average Loss = 0.00115\n",
            "Epoch#10: Train Average Loss = 0.00108\n",
            "Epoch#11: Train Average Loss = 0.00103\n",
            "Epoch#12: Train Average Loss = 0.00101\n",
            "Epoch#13: Train Average Loss = 0.00098\n",
            "Epoch#14: Train Average Loss = 0.00095\n",
            "Epoch#15: Train Average Loss = 0.00093\n",
            "Epoch#16: Train Average Loss = 0.00091\n",
            "Epoch#17: Train Average Loss = 0.00089\n",
            "Epoch#18: Train Average Loss = 0.00087\n",
            "Epoch#19: Train Average Loss = 0.00086\n",
            "Epoch#20: Train Average Loss = 0.00084\n",
            "Epoch#21: Train Average Loss = 0.00084\n",
            "Epoch#22: Train Average Loss = 0.00082\n",
            "Epoch#23: Train Average Loss = 0.00081\n",
            "Epoch#24: Train Average Loss = 0.00080\n",
            "Epoch#25: Train Average Loss = 0.00080\n",
            "Epoch#26: Train Average Loss = 0.00078\n",
            "Epoch#27: Train Average Loss = 0.00078\n",
            "Epoch#28: Train Average Loss = 0.00078\n",
            "Epoch#29: Train Average Loss = 0.00078\n",
            "Epoch#30: Train Average Loss = 0.00077\n",
            "Epoch#31: Train Average Loss = 0.00077\n",
            "Epoch#32: Train Average Loss = 0.00076\n",
            "Epoch#33: Train Average Loss = 0.00077\n",
            "Epoch#34: Train Average Loss = 0.00076\n",
            "Epoch#35: Train Average Loss = 0.00076\n",
            "Epoch#36: Train Average Loss = 0.00075\n",
            "Epoch#37: Train Average Loss = 0.00075\n",
            "Epoch#38: Train Average Loss = 0.00075\n",
            "Epoch#39: Train Average Loss = 0.00075\n",
            "Epoch#40: Train Average Loss = 0.00074\n",
            "Epoch#41: Train Average Loss = 0.00074\n",
            "Epoch#42: Train Average Loss = 0.00074\n",
            "Epoch#43: Train Average Loss = 0.00073\n",
            "Epoch#44: Train Average Loss = 0.00073\n",
            "Epoch#45: Train Average Loss = 0.00073\n",
            "Epoch#46: Train Average Loss = 0.00073\n",
            "Epoch#47: Train Average Loss = 0.00072\n",
            "Epoch#48: Train Average Loss = 0.00072\n",
            "Epoch#49: Train Average Loss = 0.00072\n",
            "Epoch#50: Train Average Loss = 0.00072\n",
            "\n",
            "Tuning the uncertainty margin...\n",
            "Optimal uncertainty margin found: 0.000009 (Resulting F1 on tune set: 0.8560)\n",
            "\n",
            "--- Final Evaluation on Test Set ---\n",
            "\n",
            "Autoencoder Performance (baseline):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.43      0.75      0.54       118\n",
            "      Attack       0.00      0.00      0.00       118\n",
            "\n",
            "    accuracy                           0.37       236\n",
            "   macro avg       0.21      0.37      0.27       236\n",
            "weighted avg       0.21      0.37      0.27       236\n",
            "\n",
            "  Acc: 0.3729, DR: 0.0000, FAR: 0.2542\n",
            "  CM:\n",
            "[[ 88  30]\n",
            " [118   0]]\n",
            "\n",
            "--- Applying Targeted Correction with Tuned Margin ---\n",
            "\n",
            "Final Hybrid Model Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      0.69      0.82       118\n",
            "      Attack       0.77      1.00      0.87       118\n",
            "\n",
            "    accuracy                           0.85       236\n",
            "   macro avg       0.88      0.85      0.84       236\n",
            "weighted avg       0.88      0.85      0.84       236\n",
            "\n",
            "  Acc: 0.8475, DR: 1.0000, FAR: 0.3051\n",
            "  CM:\n",
            "[[ 82  36]\n",
            " [  0 118]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.847457627118644,\n",
              " 1.0,\n",
              " np.float64(0.3050847457627119),\n",
              " array([[ 82,  36],\n",
              "        [  0, 118]]))"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    }
  ]
}